[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLM Agentic Systems",
    "section": "",
    "text": "Preface\nIn March of 2025, I had the new exciting opportunity to join a new team developing foundational models and AI agents. Since the field of agents is new and rapidly evolving, the number of established texts or formal courses on AI agents is sparse. A common joke in this field is that by the time someone writes a book, half of the content becomes outdated when finished. Instead, most people that I know stay up to date by continually reading blogs, tweets, and papers. However, without a comprehensive text, it can be hard to see how concepts fit together within the bigger picture. This is true especially when definitions are still being debated, new perspectives are still being developed, and new terms are being invented. A recent example is the term “context engineering”, which gained popularity after a discussion on X on June of 2025, where the term was introduced to unify the goals of prompt engineering, memory management, tool use, etc.\nI started this online book as an attempt to organize and aggregate the lessons I learned along the way as I build agents. I use the structure of a book to organize ideas and concepts coherently, and choose the format to be online to easily incorporate updates in the field, which are frequent. The contents are drawn from my learnings from research papers, blogs, talks, and the practical experience of building agents. My goal is to write in sufficient levels of depth and detail to reveal how things work “underneath the hood”, which may be difficult to see when most frameworks and agent-building tools abstract away those details. However, knowing these details is more empowering as it allows us to build more freely, concretely, and effectively, instead of attributing certain agentic abilties to “magic”. Hence, this book will have a focus on describing low-level agent mechanisms with code illustrations when appropriate. Given how new and evolving the field of AI agents are, the definitions and perspectives of this book may not necessarily align with everyone’s views or stand the test of time, nor are they meant to be comprehensive. Rather, the aim is to provide one valid mental model of how agents work to help people get started with building agents.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Prompt\nIn artifical intelligence (AI), an agent is broadly defined as anything that can perceive and act in its own environment (Norvig and Intelligence 2002). With the rise of large language models (LLMs), LLMs are now used to power modern agentic systems by leveraging their much more powerful and generalized intelligence capabilities that emerged from scale (Brown et al. 2020; Wei et al. 2022). At its best, a LLM can dynamically decide the sequence of steps that need to be executed in order to accomplish a given task, essentially achieving autonomy.\nIn practice, agentic systems differ in the degree of reliance on the LLM as a decision maker, since the increased flexibility that LLMs provide comes with the cost of reliability. On one end of this spectrum is a LLM workflow, which has LLMs participate in a limited scope within a broader predefined workflow. The steps are pre-defined and the LLM is tasked with making some of the decisions. On the other end of the spectrum is a LLM agent, where the LLM directs its own workflow to accomplish a task - deciding what and how many steps to take. We can illustrate the difference between a workflow and an agent with a customer service chatbot example:\nWhile an agent can tackle tasks more adaptively, it also becomes less predictable and reliable. On the other hand, workflows are more deterministic and reliable, but are limited in their ability to tackle more open-ended tasks where there may not be one obvious approach. Choosing between a workflow and an agent requires considering the balance of flexibility and reliability needed for the application. In the rest of this book, the word agent will be used interchangeably with agentic systems, with the distinction between workflows and agents explicitly stated when necessary.\nBuilding an agentic system from a LLM requires a prompt, tools, and memory. The prompt is piece of text that instructs the LLM on how to behave within the agent application. Tools allow an agent to take actions and is typically accessed by an agentic system in the form of an API. Finally, memory allows an agent to act and behave in a contextualized manner, with user information or conversation history being common memory contexts. Each of these components are the building blocks that can be used to create and shape a LLM agent. Figure 3.1 illustrates an agentic system and its components:\nA prompt is a piece of text that instructs a LLM how to behave within an agent application. A prompt can be organized conceptually into a system prompt, contextual prompt, role prompt, and a user prompt. In the end, they are all concatenated together into a single text input when invoking the LLM (i.e. asking LLM to generate response).\nCome LLM inference time, the process of putting together the final prompt typically involves concatenating the system prompt, one of the role prompts, the contextual prompt with contextual values filled in, and the user input. Below is an example for a bank agent chatbot, using AWS bedrock to access a LLM\nimport boto3\n\nSYSTEM_PROMMPT = \"\"\"\n&lt;instruction&gt;\nYou are a helpful agent for XYZ bank. You are ALWAYS patient, helpful, and always try to \nassist the user in the best way possible. \n&lt;/instruction&gt;\n\"\"\"\n\nROLE_PROMPT_REPORTING = \"\"\"\nYou are tasked with account reporting. \nUse the following function to look up the account information:\n\n{\n    \"function_name\": \"account_lookup\",\n    \"description\": \"a tool to retrieve account information for a user.\",\n    \"arguments\": {\n        \"username\": {\"type\": str, \"description\": \"user name\"},\n        \"security_code\": {\"type\": str, \"description\": \"security code\"}\n    }\n}\n\nNEVER reveal account Ids.\n\"\"\"\n\nCONTEXTUAL_PROMPT = \"\"\"\nUse below account information &lt;account&gt; about the customer:\n\n&lt;account&gt;\nUsername: {username}\naccount_type: {account_type}\n&lt;/account&gt;\n\"\"\"\n\nuser_input = \"Can you get the ending balance of each month for 2024?\"\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-west-1\")\nbedrock_runtime_response = bedrock_runtime.converse(\n    modelId = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    system = [\n                {'text': SYSTEM_PROMMPT}, \n                {'text': ROLE_PROMPT_REPORTING}, \n                {'text': CONTEXTUAL_PROMPT.format(username = \"caleb\", \n                                                  account_type = \"savigns\")}\n    ],\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_input}]}]\n)\nAccording to Anthropic, using XML tags in your prompts can help Claude models parse specific components in your prompt more easily. For example, better identifying which part of the prompt is the system prompt by the &lt;instruction&gt; tag. As a heuristic, capitalize words for emphasis, such as the words “NEVER” or “ALWAYS”.",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#prompt",
    "href": "introduction.html#prompt",
    "title": "1  Introduction",
    "section": "",
    "text": "System prompt: contains high level instructions that should always be applied and thus is always part of the input text when invoking a LLM. Typically, the system prompt contains instructions asking the LLM to be a helpful and patient agent.\nRole prompt: in an agentic system, LLMs may be required to behave differently depending on the scenario. For example, in multi-agent collaboration where multiple specialized agents communicate together to solve as task, each specialized agent will need a role prompt. To implement this behavior, multiple role prompts are maintained and a specific role prompt is selected and concatenated with the remaining prompts depending on the scenario or role.\nUser prompt: the question or instruction from the user of the agent application. The user prompt is typically appended to the end of the final prompt that is passed to the LLM.\nContextual prompt: catch-all prompt for all contextual details needed for an agent to respond to a user request. For industry applications, this could be the account information of the user in the current conversation session. Having a contextual prompt is important for a good and safe user experience as it saves the user from having to state user information that might be later used by the agent.",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#tools",
    "href": "introduction.html#tools",
    "title": "1  Introduction",
    "section": "1.2 Tools",
    "text": "1.2 Tools\nThe tools of an agent are the software services that a LLM can access via API calls, which gives the LLM a means to interact with the outside environment, and imbues an agent with specialized abilities. Common LLM tools include database access (for retrieval augmented generation (RAG)), web search, code interpreter, and calculator. For real-world agent applications, these tools can be specialized in-house services such as a recommendation system or placing an order.\nConcretely, a LLM “accesses” tools by generating an API call string, typically in the standardized JSON format for ease of parsing. Then, the API call string is passed to the client side, which extracts key entities like the tool name and arguments, followed by making the API call to the specified tool with the extracted arguments. While this is in principle possible with regular language models in the pre-LLM era, tool-use became more main stream as LLMs developed the instruction-following ability to generate API calls reliably if you simply provide the tool use instructions and tool documentation (e.g. tool name and required arguments) in the input prompt.\nTo illustrate the mechanism of tool-use, suppose we add to the LLM prompt the following documentation on a weather function so that the LLM knows how to generate the API call string when the user asks for the weather on a given day:\n{\n  \"name\": \"get_temperature_by_day\",\n  \"description\": \"Returns the forecasted temperature in Celsius for a specified day of the week.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"day\": {\n        \"type\": \"string\",\n        \"description\": \"Name of the day of the week (e.g., 'Monday', 'Tuesday'). Case-insensitive.\"\n      }\n    },\n    \"required\": [\"day\"]\n}\nAdditionally suppose in the prompt we instruct the model to generate the API call in JSON format for ease of parsing, for example:\n{\n    \"name\": \"get_temperature_by_day\",\n    \"arguments\": {\n        \"day\": \"Tuesday\"\n    }\n}\nThen, the code to parse and execute the function could look like:\n\nimport json\n\ndef get_temperature_by_day(day):\n    if day == \"Tuesday\":\n        return 27\n    return 30\n\ntool_call_string = \"\"\"\n{\n    \"name\": \"get_temperature_by_day\",\n    \"arguments\": {\n        \"day\": \"Tuesday\"\n    }\n}\n\"\"\"\n\n# Parsing LLM tool call string to extract tool name and argument\ntool_call_json = json.loads(tool_call_string)\nday = tool_call_json[\"arguments\"][\"day\"]\nfunc_name = tool_call_json[\"name\"]\n\n# Tool execution\ntemperature = globals()[func_name](day)\nprint(\"Temperature for {day} is {temp}C\".format(day = day, temp = temperature))\n\nTemperature for Tuesday is 27C\n\n\nFigure 3.2 shows the life cycle of a function call, and shows that the role of a LLM in tool calling is to map the user question to the corresponding tool call JSON output.\n\n\n\n\n\n\nFigure 1.2: In this example life cycle of a function call, the agent processes the user request and decides to use the Google Places API. First, example API calls are added to the prompt for in-context learning, then the LLM generates the tool call JSON payload, which gets sent to the client side to process and make the API call. The API call results are then fed to the LLM for final response generation. Source: “Agents” (https://www.kaggle.com/whitepaper-agents).",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#memory",
    "href": "introduction.html#memory",
    "title": "1  Introduction",
    "section": "1.3 Memory",
    "text": "1.3 Memory\nFinally, memory allows an agent to accumulate conversation history as context, allowing responses to become highly contextualized and efficient. Perhaps inspired by the biological mind, people like to categorize agent memory into short-term or long-term memory, with implications for usage and implementation.\nShort-term memory typically describes the conversation history of the current conversation session, which might revolve around solving a single task or topic. Like the RAM for computers, it acts as the working memory of the agent, which is typically stored in a buffer or list without further processing and passed to the LLM for each response.\nLong-term memory refers to the collection of conversation history across sessions. Since each session may concern a different topic, long-term memory is typically only accessed by an agent when relevant to the current conversation, and is thus stored in external databases that can be retrieved (e.g. vector data store for semantic retrieval). When long-term memory is retrieved for the current conversation session, it may get summarized first before passing to the LLM in order to utilize its context window efficiently. An example of this long-term memory processing is in the multi-agent collaboration of ChatDev, where each long-term memory is the conversation history between two agents for solving a subtask (Qian et al. 2023). To start the next subtask, the solution to the previous subtask is extracted from long-term memory and loaded into the LLM context.",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "context.html",
    "href": "context.html",
    "title": "2  Context Engineering",
    "section": "",
    "text": "LLMs are trending towards having longer context windows, capable of processing upwards of millions of tokens. This is regarded as beneficial overall, as larger contexts allow a model to process more information and solve more complex problems. However, research from Chroma showed that model performance degrades as context length increases, a phenomenom their research team coined “context rot” (Hong, Troynikov, and Huber 2025). Specifically, they observed this phenomenon by evaluating LLMs on the Needle in a Haystack (NIAH) task, where the LLM is instructed to answer a question where the answer (i.e. needle) is embedded in a larger, unrelated body of text (i.e. haystack). Traditionally, the needle for a question can be identified via lexical matching, or exact matching on words or phrases. For example:\n\nQuestion: Which book sparked my interest in AI?\nNeedle: The book which sparked my interest in AI is “The Worlds I see”.\n\nA needle can also be identified through semantic matching or via a LLM’s world knowledge that do not involve lexcial matching. For example:\n\nQuestion: Which professor had prior experience in theoretical research?\nNeedle: Ten years ago, Ms. Carter spent a decade at the Institute for Advanced Study.\n\nIn the above example, in order to identify the needle, the LLM had to utilize its world knowledge that the Institute for Advanced Study is a center for theoretical research as well as semantic identification of the needle’s relevance to the question. There are no overlapping words between the question and needle in this case.\nUsing the NIAH task, the Chroma team carried out a series of controlled experiments to show that performance generally degrades with longer contexts. The key findings are:\n\nWhen the task complexity is held constant by keeping the question-needle embedding cosine similarity the same, model performance degrades with context length. The performance decline tends to be more rapid for question-needle pairs that are more dissimilar.\nThe decline in performance is also influenced by distractors (i.e. text chunks that are topically related to the needle but does not answer the question), the content of the haystack, and the structural coherency of the haystack (i.e. randomly shuffled sentences or not).\nNo evidence that needle position in the haystack influences performance.\n\nFor a broader discussion on the types of long context problems, such poisoning, distraction, confusion, or clash, refer to this blog post by Drew Breunig.\nTogether, these findings suggest that managing the context of the LLM is important for effectiveness. Specifically, that the LLM’s context window should be filled with relevant content for a given task, but no more, in order to be maximally effective. This context management effort is now coined the term “context engineering”, commonly defined as “the art of providing all the context for the task to be plausibly solvable by the LLM”. Context engineering regards everything that is inputted into a LLM as context, which includes memory, prompts, information from retrieved for RAG, etc.\nA langchain blog post summarizes common context engineering patterns, which include writing context to offload information for later use, information selection for the context window, compression, and isolation. A few concrete tactics include performing RAG on tool descriptions to shrink the tool-selection space to a smaller and more relevant set of tools, periodically summarizing past conversations (implemented by Claude Code and ChatDev), and multi-agent architectures where subagents own their own isolated context so that the overall agentic system is effectively using an expanded context window.\nContext compression is a key tactic used by ChatDev to implement multi-agent collaboration for software development (Qian et al. 2023), where the conversation history between two subagents for each software development phase is summarized into the solution. This solution then serves as the start context for the next phase of subagent-to-subagent dialog to tackle another set of tasks. This intuitively makes sense because only the solutions of subproblems are necessary for solving the bigger problem, and the process for solving the subproblems is usually irrelevant.\n\nReferences\n\n\n\n\nHong, Kelly, Anton Troynikov, and Jeff Huber. 2025. “Context Rot: How Increasing Input Tokens Impacts LLM Performance.” Chroma. https://research.trychroma.com/context-rot.\n\n\nQian, Chen, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, et al. 2023. “Chatdev: Communicative Agents for Software Development.” arXiv Preprint arXiv:2307.07924.",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Context Engineering</span>"
    ]
  },
  {
    "objectID": "reflection.html",
    "href": "reflection.html",
    "title": "3  Reflection",
    "section": "",
    "text": "3.1 Self-Correction with External Feedback\nReflection is one of the most effective design patterns for agents, where a LLM enters into an iterative loop of problem solving and feedback until a solution is reached. This feedback can be external or intrinsic, where external feedback refers to receiving feedback from external evaluators such as humans, code execution output, or unit test results, and intrinsic feedback refers to self-critique from the same LLM on its previous response. The reflection process is depicted in Figure 3.1.\nThere has been some debate regarding whether self-feedback from the same LLM alone can drive performance improvements. Madaan et al. introduced Self-Refine, a reflection framework that relies completely on intrinsic feedback (Madaan et al. 2023). After the initial response construction, each iteration of Self-Refine consists of the steps of feedback and refinement. Each step uses the same LLM, but differs in the prompt that contains instructions (e.g. instructions for feedback or refinement) and few-shot examples. Empirically, this was shown to improve performance materially, with most of the improvement occurring in the initial iterations of reflection. However, the improvement is not necessarily monotonic, and Madaan et al. observed that Self-Refine was most effective when the feedback is specific, actionable, or broken down into different evaluation dimensions. However, another group of researchers from Google DeepMind independently assessed instrinsic self-correction and found that its effectiveness was limited, instead causing performance degradation with each self-refine iteration (Huang et al. 2023). They concluded that this discrepancy was caused by flawed experimental design from the Self-Refine study where the complete set of requirements was included in the feedback prompt instead of the initial response instruction. As a result, it is unclear whether the improvement was due to “leakage” of requirements from the feedback or from the iterative process of self-improvement. When the complete set of requirements was included in the initial response instruction, Huang et al. observed that standard prompting outperformed Self-Refine.\nThese studies show that in order to reliably improve after reflection, external feedback is necessary. Intuitively, this makes sense because if the bottleneck of performance is due to the lack of certain parametric knowledge (i.e. knowledge trained into the weights of a model), then feedback from the same LLM is unlikely to provide the missing knowledge required to arrive at the solution. We next introduce self-correction from external feedback and ReAct as effective reflection examples that make use of external feedback or signal to drive performance beyond prompting a LLM in isolation.\nMultiple works of research has shown that external feedback reliably and materially improves performance on the problem an agent is trying to solve, with the LLM self-correcting its previous response using the provided feedback (Chen et al. 2023; Shinn et al. 2023; Gou et al. 2023). Gou et al. introduced a simple version of this by using a set of tools through which critiques on correctness are obtained. The critiques are used by the LLM to improve its previous response, and the loop terminates when the critiques determine the latest response to be correct (Gou et al. 2023). Shinn et al. introduced the Reflexion framework that added more bells and whistles by (1) integrating LLM-driven self-reflection with external feedback signal to serve as the final feedback and (2) incorporating short-term memory of conversation history (i.e. trajectory) and long-term memory of past verbal feedback. By using an evaluator LLM to score the latest LLM response and using another LLM to suggest actionable feedback based on the score, Reflexion improves the quality of the feedback. With memory of the LLM’s past interactions with the envornment and their outcomes, the actor LLM essentially undergoes reinforcement learning based on in-context learning examples. The Reflexion framework is best illustrated with the diagram in Figure 3.2.\nFinally, Chen et al. applied reflection to programming agents to simulate rubber duck debugging, where debugging is done by explaining code and following the execution results (Chen et al. 2023). Like Reflexion, the feedback step includes both the external signal (from code execution) and a LLM generated explanation of the code as the final feedback provided to the same LLM for the next iteration of code generation. In agreement with other research, the authors found that receiving feedback from code execution is important for improving performance consistently.",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reflection</span>"
    ]
  },
  {
    "objectID": "reflection.html#self-correction-with-external-feedback",
    "href": "reflection.html#self-correction-with-external-feedback",
    "title": "3  Reflection",
    "section": "",
    "text": "Figure 3.2: Reflexion framework. Source: “Reflexion: Language Agents with Verbal Reinforcement Learning” (https://arxiv.org/pdf/2303.11366).",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reflection</span>"
    ]
  },
  {
    "objectID": "reflection.html#react",
    "href": "reflection.html#react",
    "title": "3  Reflection",
    "section": "3.2 ReAct",
    "text": "3.2 ReAct\nReAct, which stands for reasoning and acting, is a prompting technique to combine reasoning and acting with language models to solve diverse language reasoning and decision making tasks (Yao et al. 2023). By interspersing actions and subsequent observations with a LLM’s reasoning traces, Yao et al. showed that this reduces hallucination and error propagation, demonstrating superior performance over baselines like standard prompting, chain-of-thought prompting (i.e. reasoning only), and acting only (i.e. ReAct without thoughts). It is believed that reasoning improves an agent’s subsequent actions via mechanisms such as introducing notions of planning, injecting extra parametric knowledge, or by extracting important parts of observations.\nConcretely, ReAct prompting induces a repeating pattern of thinking, acting, and receiving observation until a task is solved. Starting with a question, ReAct prompting invokes the LLM to output thinking traces on how to solve the problem, followed by another LLM invocation on what action to output given the thinking. The selected action is executed, then the corresponding observation is added to the agent’s short-term memory (i.e. conversation history), and the process repeats. Thus, the number of LLM calls is \\(O(N)\\) in the number \\(N\\) of think, act, and observation iterations required to solve a problem, making ReAct prompting relatively expensive. However, ReAct improves the flexibility of an agent to address open-ended requests dynamically. Additionally, Yao et al. reported ReAct introduces strong generalization capabilities to new problems with just a few in-context examples.\nWe next use ReAct to implement an agent that uses reasoning and internet search to recommend where to live in the United States based on their preferences. Our ReAct prompt first describes to the LLM that we wish to solve the problem using a thinking, action, and observation pattern, instructing the LLM output format at each step, explaining the action space, and how to end. Then, this is followed by a one-shot example of a thought, action, and observation pattern used to address a sample question. The prompt ends by instructing the agent to only output tokens for action or thinking.\n\nSYSTEM_PROMPT = \"\"\"\nYou are an assistant trying to help me determine which city in the United States I should live in.\n\"\"\"\n\nROLE_PROMPT = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. \nRespond ONLY with ONE JSON-complaint string of the format:\n\n{{\n    \"type\": \"Thought\" or \"Action\"\n    \"content\": \"content of Thought, Action, or Observation\"\n}}\n\nThe \"content\" of Action is either \"Search[arguments]\" or \"Finish[answer]\", where:\n\n-`\"Search[arguments]\"` means to search the web with arguments\n-`\"Finish[answer]\"` means to finish the steps with an answer. You should output `\"Finish[answer]\"` as soon as you've gathered enough information\n  to answer the question.\n\nThe \"content\" of Thought is a reasoning about what Action to take next based on previous Observation. DO NOT output new lines for Thought content.\n\nExample pattern:\n\nQuestion: \n\"Which city should I live in and what activities does it provide? I enjoy walkable cities.\"\n\nThought 1:\n{{\n    \"type\": \"Thought\"\n    \"content\": \"To find which city you should live in, I need to search for a list of most walkable cities in the \n                United States and their temperatures.\"\n}}\n\nAction 1:\n{{\n    \"type\": \"Action\"\n    \"content\": \"Search[Most walkable American cities]\"\n}}\n\nObservation 1:\n{{\n    \"type\": \"Observation\"\n    \"content\": \"One of the most walkable cities in the U.S. is New York.\"\n}}\n\nThought 2:\n{{\n    \"type\": \"Thought\"\n    \"content\": \"I need to search what hobbies or activities are available in New York.\"\n}}\n\nAction 2:\n{{\n    \"type\": \"Action\"\n    \"content\": \"Search[What hobbies or activities are available in New York?]\"\n}}\n\nObservation 2:\n{{\n    \"type\": \"Observation\"\n    \"content\": \"From big highlights like Times Square to quaint walks locals love. New York City is a hub of culture \n                and adventure, offering an array of activities for everyone. Explore iconic landmarks like the Statue \n                of Liberty, the Empire State Building, and Central Park, or marvel at art of the Museum of Modern Art.\"\n}}\n\nThought 3:\n{{\n    \"type\": \"Thought\"\n    \"content\": \"From the search results, New York City is the most walkable city in America with hobbies or activites \n                like exploring city landmarks or visiting museums.\"\n}}\n\nAction 3:\n{{\n    \"type\": \"Action\"\n    \"content\": \"Finish[I recommend you live in New York City, which is one of the most walkable cities in America. There \n                       you can explore city landmarks Statue of Liberty, the Empire State Building, and Central Park, or marvel \n                       at art of the Museum of Modern Art]\"\n}}\n\n...\n\nReturn type and content for ONE step ONLY\n\"\"\"\n\nTo implement the actual agent, we essentially wrap the LLM call within a while loop until a maximum number of steps is reached. The agent class is additionally equipped with:\n\nShort-term memory, implemented simply as a string in this protype.\nThe internet search tool DuckDuckGoSearchRun, which simply accepts a natural language query and responds with a paragraph of text results.\n\nThen, in each iteration we parse the LLM response to decide what action to take. The initial question and subsequent thoughts, actions, and observations are continually added to the conversation history to influence what the LLM will output next. For example, if the last item in the history is an observation, then the LLM will output a thinking trace based on following the ReAct prompt.\n\nfrom langchain_community.tools import DuckDuckGoSearchRun\nimport json\nimport boto3\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass ReAct_agent:\n    def __init__(self, client, system_prompt, role_prompt, max_steps=20, search_max_length=1000):\n        self.client = client\n        self.SYSTEM_PROMPT = system_prompt\n        self.ROLE_PROMPT = role_prompt\n        self.ddg_search = DuckDuckGoSearchRun()\n        self.max_steps = max_steps\n        self.step_num = 0\n        self.search_max_length = search_max_length\n        self.history = \"\"\n\n    def ask(self, message, verbose = True):\n        self.add_to_memory(message)\n        model_response = self.invoke_llm(message)\n        model_response_json = json.loads(model_response.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n        response_type = model_response_json[\"type\"]\n        while self.step_num &lt; self.max_steps:\n            self.add_to_memory(model_response)\n            if response_type == \"Action\":\n                prefix, content = self.parse_action(model_response_json[\"content\"])\n                if prefix == \"Finish\":\n                    return content\n                elif prefix == \"Search\":\n                    observation = self.search(content)\n                    if verbose:\n                        print(json.dumps({\"type\": \"Observation\", \"content\": observation}, indent=4))\n                    self.add_to_memory(json.dumps({\"type\": \"Observation\", \"content\": observation}))\n            model_response = self.invoke_llm(self.history)\n            model_response_json = json.loads(model_response.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n            response_type = model_response_json[\"type\"]\n            if verbose:\n                print(json.dumps(model_response_json, indent=4))\n            self.step_num += 1\n\n    def search(self, message):\n        return self.ddg_search.invoke(message)[:self.search_max_length]\n\n    def invoke_llm(self, message):\n        bedrock_runtime_response = bedrock_runtime.converse(\n            modelId = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n            system = [\n                {'text': self.SYSTEM_PROMPT}, \n                {'text': self.ROLE_PROMPT}\n            ],\n            messages = [{\"role\": \"user\", \"content\": [{\"text\": message}]}]\n        )\n        return bedrock_runtime_response[\"output\"]['message']['content'][0]['text']\n\n    def add_to_memory(self, message):\n        self.history += \", \" + message\n\n    def parse_action(self, message):\n        start = message.find('[')\n        end = message.find(']')\n        prefix = message[:start]\n        content = message[start+1:end]\n        return prefix, content\n\nNow we ask the agent where we should live if we prefer cities with access to nature and outdoor activities.\n\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n\nagent = ReAct_agent(bedrock_runtime, SYSTEM_PROMPT, ROLE_PROMPT, max_steps = 15)\n\nuser_input = \"Which city should I live in? I enjoy outdoor activities.\"\nanswer = agent.ask(user_input)\n\nprint(answer)\n\n{\n    \"type\": \"Action\",\n    \"content\": \"Search[best US cities for outdoor activities]\"\n}\n{\n    \"type\": \"Observation\",\n    \"content\": \"Nov 16, 2024 \\u00b7 Adventure awaits in these top 10 US cities ! Discover destinations perfect for hiking, biking, skiing, and more outdoor activities across America. Mar 19, 2025 \\u00b7 15 U.S . Cities with the Best Outdoor Activities for Nature Lovers March 19, 2025 by Donna Dizon Leave a Comment Many nature lovers long to live in a cabin out in the country, but for most of us, that\\u2019s not a reality. Sep 2, 2025 \\u00b7 Read Time: 8 min read Summary: Looking for the best cities for outdoor activities in the U.S .? Check out Alexandria, VA for its beautiful parks and trails, Boston, MA for skiing and kayaking, Denver, CO for mountain adventures, Portland, OR for hiking and camping, Roseville, CA for river rafting and national parks, Salt Lake City, UT for skiing and hiking, and Seattle, WA for fishing and ... Image Editorial Credit: Farragutful via Wikimedia Commons Park City is a year-round outdoor destination famous for its ski resorts, including the largest ski area in the US . In the winter, the a\"\n}\n{\n    \"type\": \"Thought\",\n    \"content\": \"Based on the search results, I've found several cities that are known for outdoor activities. The top contenders appear to be Denver, CO; Portland, OR; Salt Lake City, UT; Seattle, WA; and Park City, UT. I should get more specific information about a few of these cities to provide better recommendations. Let me search for more details about Denver, as it's known for mountain adventures which could be appealing for outdoor enthusiasts.\"\n}\n{\n    \"type\": \"Action\",\n    \"content\": \"Search[outdoor activities in Denver Colorado]\"\n}\n{\n    \"type\": \"Observation\",\n    \"content\": \"Mar 7, 2025 \\u2014 There are tons of opportunities to go hiking, running and fly-fishing in and around the city. Denver is a paradise for cyclists with bike lanes and paved paths ... 11 hours ago \\u2014 1. Small Group Tour of Pikes Peak and the Garden of the Gods from Denver \\u00b7 2. Discover Rocky Mountain National Park \\u00b7 3. Red Rocks & Beyond \\u00b7 4 ... The conversation discusses various day trip destinations and activities in Colorado, including Tiny Town , Cheyenne Mountain Zoo, Georgetown Railroad, and South ... Apr 8, 2025 \\u2014 Denver's spring and summer months are packed with free things to do like strolling through the city's vast park system , checking out rock 'n' roll ... May 6, 2025 \\u2014 1. Tour the Colorado State Capitol \\u00b7 2. Tour the Denver Mint \\u00b7 3. Visit one of Denver's many museums \\u00b7 4. Immerse yourself in the city's art scene \\u00b7 5. Enjoy ...\"\n}\n{\n    \"type\": \"Thought\",\n    \"content\": \"Denver appears to be a great option for outdoor activities, with hiking, running, fly-fishing, cycling paths, and proximity to attractions like Rocky Mountain National Park, Pikes Peak, and Garden of the Gods. Let me check another popular option from the list - Portland, OR - to provide a comparative recommendation.\"\n}\n{\n    \"type\": \"Action\",\n    \"content\": \"Search[outdoor activities in Portland Oregon]\"\n}\n{\n    \"type\": \"Observation\",\n    \"content\": \"United States Oregon (OR) Portland Things to Do in Portland Outdoor Activities in Portland Founded by Filipina cyclocross racer Gritchelle Fallesgon and Korean-born cyclist Molly Sugar, Friends on Bikes diversifies the cycling scene by connecting and hosting rides for women and non-binary people of color. Launched in 2017, this Portland -based group organizes monthly all-levels social rides, as well as \\u201cbike-packing\\u201d trips, workshops and ... See full list on travelportland.com Dedicated to inspiring, connecting and engaging Latino communities in the outdoors, Latino Outdoors PDX organizes events and showcases how conservation roots have been ingrained in Latino cultura for generations. See full list on travelportland.com Through outings, skill session, and climb nights at local rock climbing gyms, PDX Climbers of Colorlevels the playing field by eliminating the social and economic roadblocks that typically make the sport inaccessible. See full list on travelportland.com Portland \\u2019s Pam\"\n}\n{\n    \"type\": \"Thought\",\n    \"content\": \"I've gathered information about Denver and Portland, two cities known for outdoor activities. Denver offers hiking, running, fly-fishing, cycling paths, and proximity to natural attractions like Rocky Mountain National Park. Portland has a strong cycling community and climbing opportunities. To make a more comprehensive recommendation, I should explore another top contender for outdoor activities - Seattle, WA.\"\n}\n{\n    \"type\": \"Action\",\n    \"content\": \"Search[outdoor activities in Seattle Washington]\"\n}\n{\n    \"type\": \"Observation\",\n    \"content\": \"From hiking & biking to kayaking & skiing add a dash of adventure to your vacation with these fun outdoor activities in Seattle , Washington ! Aug 28, 2025 \\u00b7 Log off, switch off and reconnect with the outdoors with these top picks of the best outdoor activities in Seattle . It's no secret that Washington boasts beautiful nature spots. Seattle itself, although it's a big city, is a hub for nature lovers with its plethora of parks, waterfront access, arboretums, and lake activities . There are so many outdoorsy things to do in Seattle , you'll get to fall in love with Washington's nature all from the convenience of staying in Seattle . What are the top water activities in Seattle ? Lake Union is the go-to spot for water activities in Seattle . Rent a kayak or paddleboard and enjoy gliding past the historic houseboats, including the one featured in \\\"Sleepless in Seattle .\\\" Lake Washington also offers opportunities for boating, paddleboarding, and wildlife spotting. Explore Seattle's stunn\"\n}\n{\n    \"type\": \"Thought\",\n    \"content\": \"I now have information about three top cities for outdoor enthusiasts: Denver, Portland, and Seattle. Each offers unique outdoor experiences. Denver has mountain activities, hiking, cycling, and proximity to natural wonders. Portland has a cycling culture and climbing opportunities. Seattle offers water activities on Lake Union and Lake Washington, hiking, biking, kayaking, and skiing. To make a well-informed recommendation, I should look at one more city from the initial list - Salt Lake City, which was mentioned as good for skiing and hiking.\"\n}\n{\n    \"type\": \"Action\",\n    \"content\": \"Search[outdoor activities in Salt Lake City Utah]\"\n}\n{\n    \"type\": \"Observation\",\n    \"content\": \"Founded in 2020, the Outdoor Market Alliance is a non-profit association dedicated to supporting community-driven initiatives. We are a collective of 25 independent sales and marketing \\u2026 Apr 15, 2025 \\u00b7 It\\u2019s often overshadowed by attractions in Denver, but the state\\u2019s fifth-largest city features fun outdoor recreation, historic sites, and stellar places to shop and dine. This guide \\u2026 REI Lakewood provides outdoor enthusiasts in the Lakewood, Colorado, area with top-brand gear & clothing for camping, climbing, cycling, fitness, hiking, paddling, skiing, snowboarding and \\u2026 Top-brand gear, clothing\\u2014and outdoor adventures! Plus rentals, classes, events, expert advice and more. Visit REI Co-op online and in-store. Explore 12 top spots with great access and hunter lodging. Outdoor Life has been the go-to publication of America\\u2019s diehard hunters, shooters, and anglers since 1898. Our stories are \\u2026\"\n}\n{\n    \"type\": \"Thought\",\n    \"content\": \"The search results for Salt Lake City didn't provide specific information about outdoor activities there, though it was mentioned in the initial search as being good for skiing and hiking. From the information I've gathered about Denver, Portland, and Seattle, I now have enough details to make a recommendation. All three cities offer excellent outdoor activities, but they have different strengths. Denver is great for mountain activities, hiking, and proximity to natural parks. Portland has a strong cycling and climbing culture. Seattle offers water activities as well as hiking, biking, and winter sports. I should now provide a recommendation based on this comprehensive information.\"\n}\n{\n    \"type\": \"Action\",\n    \"content\": \"Finish[Based on your interest in outdoor activities, I recommend Denver, Colorado as an excellent city to live in. Denver offers an exceptional variety of outdoor recreation opportunities including hiking, running, cycling on extensive bike paths, and fly-fishing in and around the city. You'll have access to world-class natural attractions like Rocky Mountain National Park, Pikes Peak, and Garden of the Gods. The city has a robust park system and enjoys abundant sunshine year-round, making it ideal for outdoor enthusiasts. Alternatives worth considering are Portland, Oregon (known for its cycling culture and climbing opportunities) and Seattle, Washington (which offers water activities like kayaking and paddleboarding on Lake Union and Lake Washington, plus nearby hiking, biking and skiing).]\"\n}\nBased on your interest in outdoor activities, I recommend Denver, Colorado as an excellent city to live in. Denver offers an exceptional variety of outdoor recreation opportunities including hiking, running, cycling on extensive bike paths, and fly-fishing in and around the city. You'll have access to world-class natural attractions like Rocky Mountain National Park, Pikes Peak, and Garden of the Gods. The city has a robust park system and enjoys abundant sunshine year-round, making it ideal for outdoor enthusiasts. Alternatives worth considering are Portland, Oregon (known for its cycling culture and climbing opportunities) and Seattle, Washington (which offers water activities like kayaking and paddleboarding on Lake Union and Lake Washington, plus nearby hiking, biking and skiing).\n\n\nWe see that with ReAct prompting, the agent first got a list of candidate cities, then searched each candidate city for specific outdoor activities before returning the final answer, which is Denver, Colorado.",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reflection</span>"
    ]
  },
  {
    "objectID": "reflection.html#llm-as-optimizers",
    "href": "reflection.html#llm-as-optimizers",
    "title": "3  Reflection",
    "section": "3.3 LLM as Optimizers",
    "text": "3.3 LLM as Optimizers\nYang et al. from DeepMind showed that a LLM can be prompted to perform non-gradient optimization of numerical and discrete problems, using linear regression and the traveling salesman problem as example problems (Yang et al. 2023). The prompt includes a description of the problem and goal, followed by a set of candidate solutions and their corresponding scores, which the authors together refer to as the meta-prompt. By proposing new solutions and getting them scored, the LLM can utilize in-context learning to iterate on the problem. Crucially, this requires an external objective function evaluator to score solutions at each iteration, which are then added to the growing set of solution-score pairs. The solution-score pairs are sorted to presumably better help the LLM identify patterns that can be used to further refine the solution. See Figure 3.3. to see how a LLM can be used as an optimizer through prompting.\n\n\n\n\n\n\nFigure 3.3: Optimization by prompting framework. Source: “Large Language Models as Optimizers” (https://arxiv.org/pdf/2309.03409).\n\n\n\nWe can use the meta-prompt for linear regression from the paper to explain the prompt design:\n\nNow you will help me minimize a function with two input variables w, b. I have some (w, b) pairs and the function values at those points. The pairs are arranged in descending order based on their function values, where lower values are better..\ninput:w=18, b=15value:10386334\ninput:w=17, b=18value:9204724\nGive me a new (w, b) pair that is different from all pairs above, and has a function value lower than any of the above. Do not write code. The output must end with a pair [w, b], where w and b are numerical values..\n\nIn the above meta-prompt, the orange text are the instructions for linear regression and the blue text are the set of candidate solutions are the weight values \\(w, b\\) along with their sum of squared error (presumably).\nAs a more practical application, Yang et al. used the LLM to optimize the prompt for a given task. Specifically, the meta-prompt contains a statement that the goal is to generate a prompt (i.e. meta-instructions), examples of the output format, and past prompt-score pairs. After generation, a scorer LLM or objective function computes accuracy scores for these new prompts. The meta-prompt is then updated with the best new prompt-score pairs for further optimization.\n\n\n\n\nChen, Xinyun, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. “Teaching Large Language Models to Self-Debug.” arXiv Preprint arXiv:2304.05128.\n\n\nGou, Zhibin, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. “Critic: Large Language Models Can Self-Correct with Tool-Interactive Critiquing.” arXiv Preprint arXiv:2305.11738.\n\n\nHuang, Jie, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. “Large Language Models Cannot Self-Correct Reasoning Yet.” arXiv Preprint arXiv:2310.01798.\n\n\nMadaan, Aman, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, et al. 2023. “Self-Refine: Iterative Refinement with Self-Feedback.” Advances in Neural Information Processing Systems 36: 46534–94.\n\n\nShinn, Noah, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. “Reflexion: Language Agents with Verbal Reinforcement Learning, 2023.” URL Https://Arxiv. Org/Abs/2303.11366 1.\n\n\nYang, Chengrun, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. “Large Language Models as Optimizers.” In The Twelfth International Conference on Learning Representations.\n\n\nYao, Shunyu, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. “React: Synergizing Reasoning and Acting in Language Models.” In International Conference on Learning Representations (ICLR).",
    "crumbs": [
      "Concepts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reflection</span>"
    ]
  }
]