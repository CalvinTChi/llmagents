{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1f989382-837a-424d-91a1-710b056aea1b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Reflection\"\n",
    "format:\n",
    "    html:\n",
    "jupyter: python3\n",
    "number-sections: true\n",
    "number-depth: 3\n",
    "bibliography: references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0cabe-bf67-4b4b-bcfd-b2d0f899a8a6",
   "metadata": {},
   "source": [
    "Reflection is one of the most effective design patterns for agents, where a LLM enters into an iterative loop of problem solving and receiving feedback until a solution is reached. This feedback can be external or intrinsic, where external feedback refers to receiving feedback from external evaluators such as humans, code execution output, or unit test results, and intrinsic feedback refers to self-critique from the same LLM on its previous response. The reflection process is depicted in @fig-1.\n",
    "\n",
    "![The reflection process of iteratively using feedback to improve LLM output. Dotted arrows indicate optionality in how the LLM output is processed into feedback, which can be provided by the same LLM (intrinsic feedback), by an external evaluator (external feedback), or both.](./pics/reflection.png){#fig-1 fig-align=\"center\" width=80%}\n",
    "\n",
    "There has been some debate regarding whether self-feedback from the same LLM alone can drive performance improvements. Madaan _et al._ introduced Self-Refine, a reflection framework that relies completely on intrinsic feedback [@madaan2023self]. After the initial response construction, each iteration of Self-Refine consists of the steps of feedback and refinement. Each step uses the same LLM, but differs in the prompt that contains instructions (e.g. instructions for feedback or refinement) and few-shot examples. Empirically, this was shown to improve performance materially, with most of the improvement occurring in the initial iterations of reflection. However, the improvement is not necessarily monotonic, and Madaan _et al._ observed that Self-Refine was most effective when the feedback is specific, actionable, or broken down into different evaluation dimensions. However, another group of researchers from Google DeepMind independently assessed instrinsic self-correction and found that its effectiveness was limited, instead causing performance degradation with each self-refine iteration [@huang2023large]. They concluded that this discrepancy was caused by flawed experimental design from the Self-Refine study where the complete set of requirements was included in the feedback prompt instead of the initial response instruction. As a result, it is unclear whether the improvement was due to \"leakage\" of requirements from the feedback or from the iterative process of self-improvement. When the complete set of requirements was included in the initial response instruction, Huang _et al._ observed that standard prompting outperformed Self-Refine. \n",
    "\n",
    "These studies show that in order to reliably improve after reflection, external feedback is necessary. Intuitively, this makes sense because if the bottleneck of performance is due to the lack of certain parametric knowledge (i.e. knowledge trained into the weights of a model), then feedback from the same LLM is unlikely to provide the missing knowledge required to arrive at the solution. We next introduce self-correction from external feedback and ReAct as effective reflection examples that make use of external feedback or external signal to drive performance beyond prompting a LLM in isolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b2f4d-a351-442e-8d57-03e380e8940a",
   "metadata": {},
   "source": [
    "## Self-Correction with External Feedback\n",
    "\n",
    "Multiple works of research has shown that external feedback reliably and materially improves performance on the problem an agent is trying to solve, with the LLM self-correcting its previous response using the provided feedback [@chen2023teaching; @shinn2023reflexion; @gou2023critic]. Gou _et al._ introduced a simple version of this by using a set of tools through which critiques on correctness are obtained. The critiques are used by the LLM to improve its previous response, and the loop terminates when the critiques determine the latest response to be correct [@gou2023critic]. Shinn _et al._ introduced the Reflexion framework that added more bells and whistles by (1) integrating LLM-driven self-reflection with external feedback signal to serve as the final feedback and (2) incorporating short-term memory of conversation history (i.e. trajectory) and long-term memory of past verbal feedback. By using an evaluator LLM to score the latest LLM response and using another LLM to suggest actionable feedback based on the score, Reflexion improves the quality of the feedback. With memory of the LLM's past interactions with the envornment and their outcomes, the actor LLM essentially undergoes reinforcement learning based on in-context learning examples. The Reflexion framework is best illustrated with the diagram in @fig-2.\n",
    "\n",
    "![Reflexion framework. Source: \"Reflexion: Language Agents with Verbal Reinforcement Learning\" (https://arxiv.org/pdf/2303.11366).](./pics/reflexion.png){#fig-2 fig-align=\"center\" width=60%}\n",
    "\n",
    "Finally, Chen _et al._ applied reflection to programming agents to simulate rubber\n",
    "duck debugging, where debugging is done by explaining code and following the execution results [@chen2023teaching]. Like Reflexion, the feedback step includes both the external signal (from code execution) and a LLM generated explanation of the code as the final feedback provided to the same LLM for the next iteration of code generation. In agreement with other research, the authors found that receiving feedback from code execution is important for improving performance consistently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19530a5d-945c-4dbd-b7fb-d47d5ad9538d",
   "metadata": {},
   "source": [
    "## ReAct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2dcc2e-3674-4684-acb0-99fd3ffd186c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
